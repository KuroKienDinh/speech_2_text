# docker compose -f docker-compose-gpu.yml -p media_process up -d --build
# docker compose -f docker-compose-gpu.yml -p media_process down -v
# print(torch.cuda.is_available())
# print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

version: "3.8"

services:
  fastapi_app:
    build:
      context: .
      dockerfile: ./api/Dockerfile-gpu
    container_name: fastapi_app
    ports:
      - "8008:8008"
    environment:
      - SERVICE=app
      - ENV=production
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - shared_temp:/app/shared_temp
    depends_on:
      - redis
      - worker

  worker:
    build:
      context: .
      dockerfile: ./api/Dockerfile-gpu
    container_name: celery_worker
    environment:
      - SERVICE=worker
      - ENV=production
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - shared_temp:/app/shared_temp
    depends_on:
      - redis

  redis:
    image: "redis:latest"
    container_name: redis_queue
    restart: always
    ports:
      - "6379:6379"

volumes:
  shared_temp:
